{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1e038a6a",
   "metadata": {},
   "source": [
    "## 1. Identify potential regions from aDNA VCF files\n",
    "\n",
    "- We have calculated this in the adna/_species_/adna_regions_contig.bed file.\n",
    "- These are regions the aDNA is confident enough to call as Reference Calls or Variant Calls.\n",
    "- These are mapped and align to the putative ancestral genome.\n",
    "- We have lifted them over to the a9 Hoiho genome, but don't need that. We can use the ancestral genome and extract that.\n",
    "- Find regions between waitaha and richdalei (the aDNA samples) that intersect, so we can use them from BEAST."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d391d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "from cyvcf2 import VCF\n",
    "\n",
    "metadata = pl.read_csv(\n",
    "    \"../Hoiho_Genomes_24Feb2024_JGG_3Pops.csv\", separator=\"\\t\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aff6e97f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample = sample_metadata.select(\"ID\").to_pandas().values[0][0]\n",
    "# Add Location and Population3 to the sample name\n",
    "# sample_metadata = sample_metadata.select(\"Location\", \"Population3\")\n",
    "# sample_metadata = sample_metadata.to_pandas()\n",
    "# sample = sample + \"_\" + sample_metadata[\"Location\"].values[0] + \"_\" + sample_metadata[\"Population3\"].values\n",
    "# Convert to str without the ndarray stuff\n",
    "# sample = str(sample[0])\n",
    "\n",
    "# For all, let's get sample id conversions\n",
    "sample_id_map = {}\n",
    "\n",
    "for row in metadata.iter_rows(named=True):\n",
    "    sample = row[\"ID\"]\n",
    "    location = row[\"Location\"]\n",
    "    population = row[\"Population3\"]\n",
    "    new_sample = f\"{sample}_{location}_{population}\"\n",
    "    sample_id_map[sample] = new_sample\n",
    "\n",
    "# Now add waitaha and richdalei\n",
    "sample_id_map[\"waitaha\"] = \"waitaha_waitaha_waitaha\"\n",
    "sample_id_map[\"richdalei\"] = \"richdalei_richdalei_richdalei\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dde2595",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all species from halstats file\n",
    "seabirds = pl.read_csv(\"../seabird_alignment_halstats\", skip_lines=4)['GenomeName'].to_list()\n",
    "seabirds = [s for s in seabirds if s is not None]\n",
    "seabirds = [s for s in seabirds if not s.startswith(\"Anc\")]\n",
    "# Filter out c90 (it's subantarctic islands and we have them from the regular pop)\n",
    "seabirds = [s for s in seabirds if not s.startswith(\"c90\")]\n",
    "seabirds = [s for s in seabirds if not s.startswith(\"a9\")] # We have a9 in the SNPs as well\n",
    "# Remove Megadyptes_antipodes\n",
    "seabirds = [s for s in seabirds if not s.startswith(\"Megadyptesantipodes\")]\n",
    "# Assert a9 is not in seabirds\n",
    "assert \"a9\" not in seabirds, \"a9 should not be in seabirds\"\n",
    "\n",
    "penguin_prefixes = (\n",
    "    \"Aptenodytes\",   # king & emperor\n",
    "    \"Spheniscus\",    # banded penguins\n",
    "    \"Pygoscelis\",    # brush‑tails\n",
    "    \"Eudyptula\",     # little penguins\n",
    "    \"Eudyptes\"       # crested penguins (includes Eudyptesmoseleyi)\n",
    ")\n",
    "\n",
    "penguins = [sp for sp in seabirds if sp.startswith(penguin_prefixes)]\n",
    "\n",
    "# For each of the seabirds, add them to the sample_id_mapping\n",
    "# The id is the same as the sample, but the mapped id we should:\n",
    "# - Remove the _genomic suffix\n",
    "# - Then repeat the 3 times == species_species_species\n",
    "for sp in seabirds:\n",
    "    if sp.endswith(\"_genomic\"):\n",
    "        mapped_id = sp[:-9]  # Remove the _genomic suffix\n",
    "    else:\n",
    "        mapped_id = sp\n",
    "    sample_id_map[sp] = f\"{mapped_id}_{mapped_id}_{mapped_id}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43f5f98a",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_columns = [\"Strands\", \"AncContig\", \"AncLength\", \"AncStart\", \"AncEnd\", \"ModernContig\", \"ModernLength\", \"ModernStart\", \"ModernEnd\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6405131",
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "\n",
    "# If your PSL has the 5-line BLAT header, set skip_rows=5; otherwise keep 0.\n",
    "skip_rows = 0  # or 5\n",
    "\n",
    "# PSL columns we need (0-based indices)\n",
    "cols = [8, 9, 10, 11, 12, 13, 14, 17, 18, 19, 20]\n",
    "names = [\n",
    "    \"Strands\", \"AncContig\", \"AncLength\", \"AncStart\", \"AncEnd\",\n",
    "    \"ModernContig\", \"ModernLength\", \"BlockCount\", \"BlockSizes\", \"QStarts\", \"TStarts\"\n",
    "]\n",
    "\n",
    "psl = pl.read_csv(\n",
    "    \"adna/waitaha/adna_regions_contig_liftover_a9.psl\",\n",
    "    separator=\"\\t\",\n",
    "    has_header=False,\n",
    "    skip_rows=skip_rows,\n",
    "    columns=cols,\n",
    "    new_columns=names,\n",
    "    infer_schema_length=0,\n",
    "    ignore_errors=True,  # tolerate occasional short lines\n",
    "    # compression=\"gzip\",  # uncomment if your PSL is gzipped\n",
    ")\n",
    "\n",
    "# Clean trailing commas, split lists, cast to int, and explode.\n",
    "# Use only features present across older Polars versions.\n",
    "waitaha_blocks = (\n",
    "    psl\n",
    "    .with_columns([\n",
    "        pl.col(\"BlockCount\").cast(pl.Int64),\n",
    "        pl.col(\"BlockSizes\").fill_null(\"\").str.replace(r',+$', '', literal=False).str.split(\",\").alias(\"bs_str\"),\n",
    "        pl.col(\"QStarts\").fill_null(\"\").str.replace(r',+$', '', literal=False).str.split(\",\").alias(\"qs_str\"),\n",
    "        pl.col(\"TStarts\").fill_null(\"\").str.replace(r',+$', '', literal=False).str.split(\",\").alias(\"ts_str\"),\n",
    "    ])\n",
    "    # If your Polars is older (<0.19), swap `.list.eval(...)` for `.arr.eval(...)` in the next 3 lines.\n",
    "    .with_columns([\n",
    "        pl.col(\"bs_str\").list.eval(pl.element().cast(pl.Int64)).alias(\"bs\"),\n",
    "        pl.col(\"qs_str\").list.eval(pl.element().cast(pl.Int64)).alias(\"qs\"),\n",
    "        pl.col(\"ts_str\").list.eval(pl.element().cast(pl.Int64)).alias(\"ts\"),\n",
    "    ])\n",
    "    .drop([\"bs_str\", \"qs_str\", \"ts_str\"])\n",
    "    .with_row_index(name=\"_rid\")          # keep the source PSL row id (handy for grouping/joins)\n",
    "    .explode([\"bs\", \"qs\", \"ts\"])          # one row per ungapped block\n",
    "    .filter(                              \n",
    "        pl.all_horizontal(\n",
    "            pl.col(\"bs\").is_not_null(),\n",
    "            pl.col(\"qs\").is_not_null(),\n",
    "            pl.col(\"ts\").is_not_null()\n",
    "        )\n",
    "    )\n",
    "    .with_columns([\n",
    "        # 0-based, half-open block intervals (PSL/BED style)\n",
    "        pl.col(\"qs\").alias(\"AncStart0\"),\n",
    "        (pl.col(\"qs\") + pl.col(\"bs\")).alias(\"AncEnd0\"),\n",
    "        pl.col(\"ts\").alias(\"ModernStart0\"),\n",
    "        (pl.col(\"ts\") + pl.col(\"bs\")).alias(\"ModernEnd0\"),\n",
    "        # 1-based starts (useful for VCF POS)\n",
    "        (pl.col(\"qs\") + 1).alias(\"AncStart1\"),\n",
    "        (pl.col(\"ts\") + 1).alias(\"ModernStart1\"),\n",
    "        (pl.col(\"qs\") + pl.col(\"bs\") + 1).alias(\"AncEnd1\"),\n",
    "        (pl.col(\"ts\") + pl.col(\"bs\") + 1).alias(\"ModernEnd1\"),\n",
    "    ])\n",
    "    .select([\n",
    "        \"Strands\", \"AncContig\", \"AncLength\", \"AncStart\", \"AncEnd\",\n",
    "        \"ModernContig\", \"ModernLength\", \"BlockCount\",\n",
    "        \"AncStart0\", \"AncEnd0\", \"ModernStart0\", \"ModernEnd0\",\n",
    "        \"AncStart1\", \"ModernStart1\", \"ModernEnd1\",\n",
    "        \"bs\", \"_rid\",\n",
    "    ])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea55015a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "\n",
    "# If your PSL has the 5-line BLAT header, set skip_rows=5; otherwise keep 0.\n",
    "skip_rows = 0  # or 5\n",
    "\n",
    "# PSL columns we need (0-based indices)\n",
    "cols = [8, 9, 10, 11, 12, 13, 14, 17, 18, 19, 20]\n",
    "names = [\n",
    "    \"Strands\", \"AncContig\", \"AncLength\", \"AncStart\", \"AncEnd\",\n",
    "    \"ModernContig\", \"ModernLength\", \"BlockCount\", \"BlockSizes\", \"QStarts\", \"TStarts\"\n",
    "]\n",
    "\n",
    "psl = pl.read_csv(\n",
    "    \"adna/richdalei/adna_regions_contig_liftover_a9.psl\",\n",
    "    separator=\"\\t\",\n",
    "    has_header=False,\n",
    "    skip_rows=skip_rows,\n",
    "    columns=cols,\n",
    "    new_columns=names,\n",
    "    infer_schema_length=0,\n",
    "    ignore_errors=True,  # tolerate occasional short lines\n",
    "    # compression=\"gzip\",  # uncomment if your PSL is gzipped\n",
    ")\n",
    "\n",
    "# Clean trailing commas, split lists, cast to int, and explode.\n",
    "# Use only features present across older Polars versions.\n",
    "richdalei_blocks = (\n",
    "    psl\n",
    "    .with_columns([\n",
    "        pl.col(\"BlockCount\").cast(pl.Int64),\n",
    "        pl.col(\"BlockSizes\").fill_null(\"\").str.replace(r',+$', '', literal=False).str.split(\",\").alias(\"bs_str\"),\n",
    "        pl.col(\"QStarts\").fill_null(\"\").str.replace(r',+$', '', literal=False).str.split(\",\").alias(\"qs_str\"),\n",
    "        pl.col(\"TStarts\").fill_null(\"\").str.replace(r',+$', '', literal=False).str.split(\",\").alias(\"ts_str\"),\n",
    "    ])\n",
    "    # If your Polars is older (<0.19), swap `.list.eval(...)` for `.arr.eval(...)` in the next 3 lines.\n",
    "    .with_columns([\n",
    "        pl.col(\"bs_str\").list.eval(pl.element().cast(pl.Int64)).alias(\"bs\"),\n",
    "        pl.col(\"qs_str\").list.eval(pl.element().cast(pl.Int64)).alias(\"qs\"),\n",
    "        pl.col(\"ts_str\").list.eval(pl.element().cast(pl.Int64)).alias(\"ts\"),\n",
    "    ])\n",
    "    .drop([\"bs_str\", \"qs_str\", \"ts_str\"])\n",
    "    .with_row_index(name=\"_rid\")          # keep the source PSL row id (handy for grouping/joins)\n",
    "    .explode([\"bs\", \"qs\", \"ts\"])          # one row per ungapped block\n",
    "    .filter(                              \n",
    "        pl.all_horizontal(\n",
    "            pl.col(\"bs\").is_not_null(),\n",
    "            pl.col(\"qs\").is_not_null(),\n",
    "            pl.col(\"ts\").is_not_null()\n",
    "        )\n",
    "    )\n",
    "    .with_columns([\n",
    "        # 0-based, half-open block intervals (PSL/BED style)\n",
    "        pl.col(\"qs\").alias(\"AncStart0\"),\n",
    "        (pl.col(\"qs\") + pl.col(\"bs\")).alias(\"AncEnd0\"),\n",
    "        pl.col(\"ts\").alias(\"ModernStart0\"),\n",
    "        (pl.col(\"ts\") + pl.col(\"bs\")).alias(\"ModernEnd0\"),\n",
    "        # 1-based starts (useful for VCF POS)\n",
    "        (pl.col(\"qs\") + 1).alias(\"AncStart1\"),\n",
    "        (pl.col(\"ts\") + 1).alias(\"ModernStart1\"),\n",
    "        (pl.col(\"qs\") + pl.col(\"bs\") + 1).alias(\"AncEnd1\"),\n",
    "        (pl.col(\"ts\") + pl.col(\"bs\") + 1).alias(\"ModernEnd1\"),\n",
    "    ])\n",
    "    .select([\n",
    "        \"Strands\", \"AncContig\", \"AncLength\", \"AncStart\", \"AncEnd\",\n",
    "        \"ModernContig\", \"ModernLength\", \"BlockCount\",\n",
    "        \"AncStart0\", \"AncEnd0\", \"ModernStart0\", \"ModernEnd0\",\n",
    "        \"AncStart1\", \"ModernStart1\", \"ModernEnd1\",\n",
    "        \"bs\", \"_rid\",\n",
    "    ])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbb857df",
   "metadata": {},
   "source": [
    "# Waitaha Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9198e5ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open up the isec file\n",
    "isec_vcf_file = \"adna/waitaha/isec/0002.vcf\"\n",
    "isec_vcf = VCF(isec_vcf_file)\n",
    "\n",
    "# As a tuple (chr, start, end)\n",
    "waitaha_isec_regions = []\n",
    "\n",
    "for variant in isec_vcf:\n",
    "    contig = variant.CHROM\n",
    "    pos = variant.POS\n",
    "\n",
    "    # If the variant is in a region for waitaha_blocks, add it to waitaha_isec_regions\n",
    "    hit_blocks = waitaha_blocks.filter(\n",
    "        (pl.col(\"ModernContig\") == contig) &\n",
    "        (pl.col(\"ModernStart1\") <= pos) &\n",
    "        (pos < pl.col(\"ModernEnd1\"))\n",
    "    )\n",
    "    if hit_blocks.height > 0:\n",
    "        # Get the first hit block (there should only be one)\n",
    "        hit_block = hit_blocks[0]\n",
    "        start = hit_block[\"ModernStart1\"][0]\n",
    "        end = hit_block[\"ModernEnd1\"][0]\n",
    "        waitaha_isec_regions.append((contig, start, end))\n",
    "\n",
    "# Get unique\n",
    "waitaha_isec_regions = list(set(waitaha_isec_regions))\n",
    "print(len(waitaha_isec_regions), \"unique regions found in isec VCF for waitaha\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daab8a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "waitaha_isec_regions[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4328d3de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's keep only regions >= 200bp\n",
    "waitaha_isec_regions = [\n",
    "    region for region in waitaha_isec_regions if (region[2] - region[1]) >= 200\n",
    "]\n",
    "print(len(waitaha_isec_regions), \"regions >= 200bp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a99a5a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# Great, randomly choose 40 of these\n",
    "waitaha_regions = random.sample(waitaha_isec_regions, 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dee328c",
   "metadata": {},
   "source": [
    "## Let's extract some regions and export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b35b6c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from needletail import (NeedletailError, Record, parse_fastx_file, reverse_complement)\n",
    "\n",
    "# # We need to get regions that are:\n",
    "# # - Present in both waitaha and richdalei (intersections)\n",
    "# # - Regions found in waitaha\n",
    "# # - Regions found in richdalei\n",
    "# # - Then any others can be from anywhere on the modern genome (provided they have SNPs)\n",
    "\n",
    "# num_regions = 400\n",
    "\n",
    "# n_waitaha = int(num_regions * 0.2)\n",
    "# n_richdalei = int(num_regions * 0.2)\n",
    "# n_other = num_regions - (n_waitaha + n_richdalei)\n",
    "# [n_waitaha, n_richdalei, n_other]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0e3296c",
   "metadata": {},
   "outputs": [],
   "source": [
    "vcf_file = \"../merged.a9.filtered.qual20_fmissing0.2.2alleles.snpsonly.pp6.19.removed.vcf.gz\"\n",
    "vcf = VCF(vcf_file)\n",
    "samples = vcf.samples\n",
    "len(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba6ae0c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We do it this way because we can only confidently call even reference alleles in aDNA in a very limited regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91e97eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load up the modern genome and extract some regions\n",
    "# Let's pull this from VCF tools snp density VCFtools version 0.1.16\n",
    "snpden = pl.read_csv(\"out.snpden\", separator=\"\\t\", has_header=True)\n",
    "\n",
    "# Get min, max, mean, stddev of SNP Density\n",
    "# Column we want is SNP_COUNT (or VARIANTS/KB), but they are equivalent for our purposes\n",
    "# Bin size is 5kbp\n",
    "snpden_stats = snpden.select([\n",
    "    pl.col(\"SNP_COUNT\").min().alias(\"min_SNP_COUNT\"),\n",
    "    pl.col(\"SNP_COUNT\").max().alias(\"max_SNP_COUNT\"),\n",
    "    pl.col(\"SNP_COUNT\").mean().alias(\"mean_SNP_COUNT\"),\n",
    "    pl.col(\"SNP_COUNT\").std().alias(\"std_SNP_COUNT\")\n",
    "])\n",
    "\n",
    "# Let's filter to SNP_COUNT > 2, and max of mean + 2.5*stddev\n",
    "snpden_filtered = snpden.filter(\n",
    "    (pl.col(\"SNP_COUNT\") > 2) &\n",
    "    (pl.col(\"SNP_COUNT\") < (snpden_stats[\"mean_SNP_COUNT\"][0] + 2.5 * snpden_stats[\"std_SNP_COUNT\"][0]))\n",
    ")\n",
    "\n",
    "# Mean, min, max, stddev of SNP_COUNT before and after filtering\n",
    "[len(snpden), len(snpden_filtered), \n",
    " snpden_stats[\"min_SNP_COUNT\"][0], snpden_stats[\"max_SNP_COUNT\"][0], snpden_stats[\"mean_SNP_COUNT\"][0], snpden_stats[\"std_SNP_COUNT\"][0],\n",
    " snpden_filtered.select([\n",
    "     pl.col(\"SNP_COUNT\").min().alias(\"min_SNP_COUNT\"),\n",
    "     pl.col(\"SNP_COUNT\").max().alias(\"max_SNP_COUNT\"),\n",
    "     pl.col(\"SNP_COUNT\").mean().alias(\"mean_SNP_COUNT\"),\n",
    "     pl.col(\"SNP_COUNT\").std().alias(\"std_SNP_COUNT\")\n",
    " ]),\n",
    " # And the 2.5*stddev threshold\n",
    " snpden_stats[\"mean_SNP_COUNT\"][0] + 2.5 * snpden_stats[\"std_SNP_COUNT\"][0]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90cc826f",
   "metadata": {},
   "outputs": [],
   "source": [
    "modern_lines = []\n",
    "\n",
    "for (contig, start, end) in waitaha_regions:  # or .iter_rows(named=True) for pandas\n",
    "    modern = f\"{contig}:{start + 1}-{end}\"\n",
    "    modern_lines.append(modern)\n",
    "\n",
    "with open(\"modern_regions.txt\", \"w\") as f: f.write(\"\\n\".join(modern_lines) + \"\\n\")\n",
    "\n",
    "# Generates a multi-FASTA with one entry per region in anc_regions.txt (same order)\n",
    "! samtools faidx -r modern_regions.txt ../a9_genome_masked.fa \\\n",
    "  | bcftools consensus -s filtered2 -H I -M N adna/waitaha/aDNA_on_mod_sorted.bcf \\\n",
    "  > out/waitaha.multi.fa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0701b56b",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(waitaha_regions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "790bc126",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import subprocess\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "# ---- inputs ----\n",
    "regions_file = \"modern_regions.txt\"\n",
    "reference_fa = \"../a9_genome_masked.fa\"\n",
    "out_dir = Path(\"out\")\n",
    "max_workers = 8  # same as -j 8\n",
    "\n",
    "# ---- setup ----\n",
    "out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def run_consensus_for_sample(sample: str) -> str:\n",
    "    \"\"\"\n",
    "    samtools faidx -r modern_regions.txt ../a9_genome_masked.fa \\\n",
    "      | bcftools consensus -s {sample} -H I -M N {vcf_file} \\\n",
    "      > out/{sample}.multi.fa\n",
    "    \"\"\"\n",
    "    out_path = out_dir / f\"{sample}.multi.fa\"\n",
    "\n",
    "    sam_cmd = [\"samtools\", \"faidx\", \"-r\", regions_file, reference_fa]\n",
    "    bcf_cmd = [\"bcftools\", \"consensus\", \"-s\", sample, \"-H\", \"I\", \"-M\", \"N\", vcf_file]\n",
    "\n",
    "    # Open output file for bcftools' stdout\n",
    "    with open(out_path, \"wb\") as fout:\n",
    "        # samtools produces FASTA to stdout\n",
    "        p1 = subprocess.Popen(\n",
    "            sam_cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE\n",
    "        )\n",
    "        # bcftools reads FASTA on stdin and writes consensus to stdout\n",
    "        p2 = subprocess.Popen(\n",
    "            bcf_cmd, stdin=p1.stdout, stdout=fout, stderr=subprocess.PIPE\n",
    "        )\n",
    "        # Allow p1 to get SIGPIPE if p2 exits early\n",
    "        p1.stdout.close()  # type: ignore[attr-defined]\n",
    "\n",
    "        # Wait and collect stderr safely\n",
    "        _, err2 = p2.communicate()\n",
    "        _, err1 = p1.communicate()\n",
    "\n",
    "        if p1.returncode != 0:\n",
    "            raise RuntimeError(\n",
    "                f\"[{sample}] samtools faidx failed (code {p1.returncode}).\\n{err1.decode(errors='ignore')}\"\n",
    "            )\n",
    "        if p2.returncode != 0:\n",
    "            raise RuntimeError(\n",
    "                f\"[{sample}] bcftools consensus failed (code {p2.returncode}).\\n{err2.decode(errors='ignore')}\"\n",
    "            )\n",
    "\n",
    "    return str(out_path)\n",
    "\n",
    "# ---- run in parallel ----\n",
    "errors = []\n",
    "print(f\"Running consensus for {len(samples)} samples with {max_workers} workers…\")\n",
    "with ThreadPoolExecutor(max_workers=max_workers) as ex:\n",
    "    futs = {ex.submit(run_consensus_for_sample, s): s for s in samples}\n",
    "    for fut in as_completed(futs):\n",
    "        sample = futs[fut]\n",
    "        try:\n",
    "            outp = fut.result()\n",
    "            print(f\"✓ {sample} → {outp}\")\n",
    "        except Exception as e:\n",
    "            errors.append((sample, str(e)))\n",
    "            print(f\"✗ {sample} failed: {e}\")\n",
    "\n",
    "if errors:\n",
    "    print(\"\\nSome samples failed:\")\n",
    "    for s, msg in errors:\n",
    "        print(f\"- {s}: {msg}\")\n",
    "    raise SystemExit(1)\n",
    "else:\n",
    "    print(\"\\nAll samples completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72543af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json, glob\n",
    "\n",
    "# Assumes you already created:\n",
    "# - anc_regions.txt (one region per line, ordered)\n",
    "# - modern_regions.txt (one region per line, ordered)\n",
    "# - out/waitaha.multi.fa (aDNA consensus across all anc regions)\n",
    "# - out/{sample}.multi.fa for each modern sample (consensus across all modern regions)\n",
    "#\n",
    "# Also assumes you saved:\n",
    "# - strand_flags.json  -> list like [\"+-\", \"++\", ...] in the same row order\n",
    "# - coords.json        -> list of pairs [(anc_str, modern_str), ...] same row order\n",
    "\n",
    "# Load Waitaha multi-FASTA in order\n",
    "waitaha_records = list(parse_fastx_file(\"out/waitaha.multi.fa\"))  # your existing parser\n",
    "\n",
    "# Load modern multi-FASTAs, one per sample\n",
    "modern_records = {}  # sample -> list of records (same length/order as waitaha_records)\n",
    "for p in glob.glob(\"out/*.multi.fa\"):\n",
    "    base = os.path.basename(p)\n",
    "    name = base.replace(\".multi.fa\", \"\")\n",
    "    if name.lower() == \"waitaha\":\n",
    "        continue\n",
    "    modern_records[name] = list(parse_fastx_file(p))\n",
    "\n",
    "# Sanity checks\n",
    "n = len(waitaha_records)\n",
    "\n",
    "os.makedirs(\"regions\", exist_ok=True)\n",
    "\n",
    "for i, region in enumerate(waitaha_regions):\n",
    "    modern_str = f\"{region[0]}:{region[1] + 1}-{region[2]}\"\n",
    "    d = f\"regions/region_{i}\"\n",
    "    os.makedirs(d, exist_ok=True)\n",
    "\n",
    "    # coordinates.txt (anc line first, modern line second)\n",
    "    with open(f\"{d}/coordinates.txt\", \"w\") as f:\n",
    "        f.write(modern_str + \"\\n\")\n",
    "\n",
    "    # Waitaha sequence (RC only if '+-')\n",
    "    wrec = waitaha_records[i]\n",
    "    wseq = wrec.seq  # your parser yields the seq string (no newlines)\n",
    "\n",
    "    with open(f\"{d}/waitaha.fasta\", \"w\") as f:\n",
    "        f.write(\">waitaha_waitaha_waitaha\\n\")\n",
    "        f.write(wseq + \"\\n\")  # unwrapped\n",
    "\n",
    "    # Modern sequences (no RC), one file per sample, header is >{sample}\n",
    "    for sample, recs in modern_records.items():\n",
    "        mseq = recs[i].seq\n",
    "        # Get sample from sample_id_map\n",
    "        sample = sample_id_map.get(sample, sample)  # Fallback to original if not\n",
    "        with open(f\"{d}/{sample}.fasta\", \"w\") as f:\n",
    "            f.write(f\">{sample}\\n\")\n",
    "            f.write(mseq + \"\\n\")  # unwrapped\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edca5746",
   "metadata": {},
   "source": [
    "# Richdalei Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f9c2631",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open up the isec file\n",
    "isec_vcf_file = \"adna/richdalei/isec/0002.vcf\"\n",
    "isec_vcf = VCF(isec_vcf_file)\n",
    "\n",
    "# As a tuple (chr, start, end)\n",
    "richdalei_isec_regions = []\n",
    "\n",
    "for variant in isec_vcf:\n",
    "    contig = variant.CHROM\n",
    "    pos = variant.POS\n",
    "\n",
    "    # If the variant is in a region for richdalei_blocks, add it to richdalei_isec_regions\n",
    "    hit_blocks = richdalei_blocks.filter(\n",
    "        (pl.col(\"ModernContig\") == contig) &\n",
    "        (pl.col(\"ModernStart1\") <= pos) &\n",
    "        (pos < pl.col(\"ModernEnd1\"))\n",
    "    )\n",
    "    if hit_blocks.height > 0:\n",
    "        # Get the first hit block (there should only be one)\n",
    "        hit_block = hit_blocks[0]\n",
    "        start = hit_block[\"ModernStart1\"][0]\n",
    "        end = hit_block[\"ModernEnd1\"][0]\n",
    "        richdalei_isec_regions.append((contig, start, end))\n",
    "\n",
    "# Get unique\n",
    "richdalei_isec_regions = list(set(richdalei_isec_regions))\n",
    "print(len(richdalei_isec_regions), \"unique regions found in isec VCF for richdalei\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73d4b025",
   "metadata": {},
   "outputs": [],
   "source": [
    "      \n",
    "import polars as pl\n",
    "from cyvcf2 import VCF\n",
    "\n",
    "# --- 1. Prepare the Blocks DataFrame ---\n",
    "# Assuming richdalei_blocks is already a Polars DataFrame like this:\n",
    "# richdalei_blocks = pl.DataFrame({\n",
    "#     \"ModernContig\": [\"chr1\", \"chr1\", \"chr2\"],\n",
    "#     \"ModernStart1\": [100, 500, 200],\n",
    "#     \"ModernEnd1\": [200, 600, 300],\n",
    "# })\n",
    "\n",
    "# For the join, it's best to rename the columns to match the variants DataFrame.\n",
    "# Also, sort the data, which is a requirement for join_asof.\n",
    "blocks_df = richdalei_blocks.rename({\n",
    "    \"ModernContig\": \"CHROM\",\n",
    "    \"ModernStart1\": \"START\",\n",
    "    \"ModernEnd1\": \"END\"\n",
    "}).sort(\"CHROM\", \"START\")\n",
    "\n",
    "\n",
    "# --- 2. Load VCF Variants into a Polars DataFrame ---\n",
    "# Instead of iterating and processing, we'll just extract the necessary data.\n",
    "isec_vcf_file = \"adna/richdalei/isec/0002.vcf\"\n",
    "isec_vcf = VCF(isec_vcf_file)\n",
    "\n",
    "# This list comprehension is much faster than a for-loop with processing inside.\n",
    "variants_data = [\n",
    "    {\"CHROM\": variant.CHROM, \"POS\": variant.POS}\n",
    "    for variant in isec_vcf\n",
    "]\n",
    "\n",
    "# If the VCF is very large and causes memory issues, you can process it in chunks.\n",
    "# But for most cases, this is fine and very fast.\n",
    "variants_df = pl.DataFrame(variants_data).sort(\"CHROM\", \"POS\")\n",
    "\n",
    "\n",
    "# --- 3. Perform the High-Speed `join_asof` ---\n",
    "# This is the core of the solution. `join_asof` finds the last block `START`\n",
    "# that is less than or equal to the variant `POS`, for each chromosome.\n",
    "found_blocks = variants_df.join_asof(\n",
    "    blocks_df,\n",
    "    left_on=\"POS\",\n",
    "    right_on=\"START\",\n",
    "    by=\"CHROM\",\n",
    "    strategy=\"backward\" # \"backward\" means find the last match before the position\n",
    ")\n",
    "\n",
    "# --- 4. Filter and Get Unique Regions ---\n",
    "# The join_asof gives us the candidate block. Now we must filter to ensure\n",
    "# the position is actually within the block's bounds (POS < END).\n",
    "# Then, we select the block columns and get the unique ones.\n",
    "richdalei_isec_regions_df = (\n",
    "    found_blocks\n",
    "    .filter(pl.col(\"END\").is_not_null()) # Remove variants that found no block\n",
    "    .filter(pl.col(\"POS\") < pl.col(\"END\")) # The crucial interval condition\n",
    "    .select([\"CHROM\", \"START\", \"END\"])   # Select the columns defining a region\n",
    "    .unique()                            # Get the unique regions\n",
    ")\n",
    "\n",
    "print(f\"{richdalei_isec_regions_df.height} unique regions found in isec VCF for richdalei\")\n",
    "\n",
    "# If you absolutely need the final output as a list of tuples:\n",
    "# richdalei_isec_regions = list(richdalei_isec_regions_df.iter_rows())\n",
    "# print(richdalei_isec_regions[:5])\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac332f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "richdalei_isec_regions[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "824f3175",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's keep only regions >= 200bp\n",
    "richdalei_isec_regions = [\n",
    "    region for region in richdalei_isec_regions if (region[2] - region[1]) >= 2000\n",
    "]\n",
    "print(len(richdalei_isec_regions), \"regions >= 2000bp\")\n",
    "\n",
    "import random\n",
    "\n",
    "# Great, randomly choose 40 of these\n",
    "richdalei_regions = random.sample(richdalei_isec_regions, 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71bd62b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "modern_lines = []\n",
    "\n",
    "for (contig, start, end) in richdalei_regions:  # or .iter_rows(named=True) for pandas\n",
    "    modern = f\"{contig}:{start + 1}-{end}\"\n",
    "    modern_lines.append(modern)\n",
    "\n",
    "with open(\"modern_regions.txt\", \"w\") as f: f.write(\"\\n\".join(modern_lines) + \"\\n\")\n",
    "\n",
    "# Generates a multi-FASTA with one entry per region in anc_regions.txt (same order)\n",
    "! samtools faidx -r modern_regions.txt ../a9_genome_masked.fa \\\n",
    "  | bcftools consensus -s with_rg -H I -M N adna/richdalei/richdalei_sorted.bcf \\\n",
    "  > out_richdalei/richdalei.multi.fa\n",
    "\n",
    "len(richdalei_regions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eb47b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import subprocess\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "# ---- inputs ----\n",
    "regions_file = \"modern_regions.txt\"\n",
    "reference_fa = \"../a9_genome_masked.fa\"\n",
    "out_dir = Path(\"out_richdalei\")\n",
    "max_workers = 8  # same as -j 8\n",
    "\n",
    "# ---- setup ----\n",
    "out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def run_consensus_for_sample(sample: str) -> str:\n",
    "    \"\"\"\n",
    "    samtools faidx -r modern_regions.txt ../a9_genome_masked.fa \\\n",
    "      | bcftools consensus -s {sample} -H I -M N {vcf_file} \\\n",
    "      > out_richdalei/{sample}.multi.fa\n",
    "    \"\"\"\n",
    "    out_path = out_dir / f\"{sample}.multi.fa\"\n",
    "\n",
    "    sam_cmd = [\"samtools\", \"faidx\", \"-r\", regions_file, reference_fa]\n",
    "    bcf_cmd = [\"bcftools\", \"consensus\", \"-s\", sample, \"-H\", \"I\", \"-M\", \"N\", vcf_file]\n",
    "\n",
    "    # Open output file for bcftools' stdout\n",
    "    with open(out_path, \"wb\") as fout:\n",
    "        # samtools produces FASTA to stdout\n",
    "        p1 = subprocess.Popen(\n",
    "            sam_cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE\n",
    "        )\n",
    "        # bcftools reads FASTA on stdin and writes consensus to stdout\n",
    "        p2 = subprocess.Popen(\n",
    "            bcf_cmd, stdin=p1.stdout, stdout=fout, stderr=subprocess.PIPE\n",
    "        )\n",
    "        # Allow p1 to get SIGPIPE if p2 exits early\n",
    "        p1.stdout.close()  # type: ignore[attr-defined]\n",
    "\n",
    "        # Wait and collect stderr safely\n",
    "        _, err2 = p2.communicate()\n",
    "        _, err1 = p1.communicate()\n",
    "\n",
    "        if p1.returncode != 0:\n",
    "            raise RuntimeError(\n",
    "                f\"[{sample}] samtools faidx failed (code {p1.returncode}).\\n{err1.decode(errors='ignore')}\"\n",
    "            )\n",
    "        if p2.returncode != 0:\n",
    "            raise RuntimeError(\n",
    "                f\"[{sample}] bcftools consensus failed (code {p2.returncode}).\\n{err2.decode(errors='ignore')}\"\n",
    "            )\n",
    "\n",
    "    return str(out_path)\n",
    "\n",
    "# ---- run in parallel ----\n",
    "errors = []\n",
    "print(f\"Running consensus for {len(samples)} samples with {max_workers} workers…\")\n",
    "with ThreadPoolExecutor(max_workers=max_workers) as ex:\n",
    "    futs = {ex.submit(run_consensus_for_sample, s): s for s in samples}\n",
    "    for fut in as_completed(futs):\n",
    "        sample = futs[fut]\n",
    "        try:\n",
    "            outp = fut.result()\n",
    "            print(f\"✓ {sample} → {outp}\")\n",
    "        except Exception as e:\n",
    "            errors.append((sample, str(e)))\n",
    "            print(f\"✗ {sample} failed: {e}\")\n",
    "\n",
    "if errors:\n",
    "    print(\"\\nSome samples failed:\")\n",
    "    for s, msg in errors:\n",
    "        print(f\"- {s}: {msg}\")\n",
    "    raise SystemExit(1)\n",
    "else:\n",
    "    print(\"\\nAll samples completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f0ebc5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json, glob\n",
    "\n",
    "# Assumes you already created:\n",
    "# - anc_regions.txt (one region per line, ordered)\n",
    "# - modern_regions.txt (one region per line, ordered)\n",
    "# - out/waitaha.multi.fa (aDNA consensus across all anc regions)\n",
    "# - out/{sample}.multi.fa for each modern sample (consensus across all modern regions)\n",
    "#\n",
    "# Also assumes you saved:\n",
    "# - strand_flags.json  -> list like [\"+-\", \"++\", ...] in the same row order\n",
    "# - coords.json        -> list of pairs [(anc_str, modern_str), ...] same row order\n",
    "\n",
    "# Load Waitaha multi-FASTA in order\n",
    "richdalei_records = list(parse_fastx_file(\"out_richdalei/richdalei.multi.fa\"))  # your existing parser\n",
    "\n",
    "# Load modern multi-FASTAs, one per sample\n",
    "modern_records = {}  # sample -> list of records (same length/order as waitaha_records)\n",
    "for p in glob.glob(\"out_richdalei/*.multi.fa\"):\n",
    "    base = os.path.basename(p)\n",
    "    name = base.replace(\".multi.fa\", \"\")\n",
    "    if name.lower() == \"richdalei\":\n",
    "        continue\n",
    "    modern_records[name] = list(parse_fastx_file(p))\n",
    "\n",
    "# Sanity checks\n",
    "n = len(richdalei_regions)\n",
    "if n > 100:\n",
    "    print(f\"Warning: {n} richdalei regions found, expected 80. Check your input files.\")\n",
    "\n",
    "\n",
    "os.makedirs(\"regions\", exist_ok=True)\n",
    "\n",
    "for i, region in enumerate(richdalei_regions):\n",
    "    i = i\n",
    "    modern_str = f\"{region[0]}:{region[1] + 1}-{region[2]}\"\n",
    "    d = f\"regions/region_{i+80}\"\n",
    "    os.makedirs(d, exist_ok=True)\n",
    "\n",
    "    # coordinates.txt (anc line first, modern line second)\n",
    "    with open(f\"{d}/coordinates.txt\", \"w\") as f:\n",
    "        f.write(modern_str + \"\\n\")\n",
    "\n",
    "    wrec = richdalei_records[i]\n",
    "\n",
    "    wseq = wrec.seq  # your parser yields the seq string (no newlines)\n",
    "\n",
    "    with open(f\"{d}/richdalei.fasta\", \"w\") as f:\n",
    "        f.write(\">richdalei_richdalei_richdalei\\n\")\n",
    "        f.write(wseq + \"\\n\")  # unwrapped\n",
    "\n",
    "    # Modern sequences (no RC), one file per sample, header is >{sample}\n",
    "    for sample, recs in modern_records.items():\n",
    "        mseq = recs[i].seq\n",
    "        # Get sample from sample_id_map\n",
    "        sample = sample_id_map.get(sample, sample)  # Fallback to original if not\n",
    "        with open(f\"{d}/{sample}.fasta\", \"w\") as f:\n",
    "            f.write(f\">{sample}\\n\")\n",
    "            f.write(mseq + \"\\n\")  # unwrapped\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67040908",
   "metadata": {},
   "source": [
    "# STOP\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f0790cc",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "afa0da7b",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b86770f8",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf3eb586",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json, glob, subprocess\n",
    "from pathlib import Path\n",
    "\n",
    "def write_regions_files(df, anc_path, modern_path, *, one_based=True):\n",
    "    \"\"\"Emit anc and modern region lists (if columns exist), preserve row order.\n",
    "       Returns (anc_regions_file or None, modern_regions_file or None,\n",
    "                coords:list[(anc_str or None, modern_str or None)],\n",
    "                strands:list or None)\"\"\"\n",
    "    anc_lines, modern_lines = [], []\n",
    "    coords = []\n",
    "    strands = df[\"Strands\"].to_list() if \"Strands\" in df.columns else None\n",
    "\n",
    "    for row in df.to_dicts():\n",
    "        anc_str = None\n",
    "        if all(c in row for c in (\"AncContig\",\"AncStart\",\"AncEnd\")):\n",
    "            s = row[\"AncStart\"] + (1 if one_based else 0)\n",
    "            e = row[\"AncEnd\"]\n",
    "            anc_str = f\"{row['AncContig']}:{s}-{e}\"\n",
    "            anc_lines.append(anc_str)\n",
    "\n",
    "        modern_str = None\n",
    "        if all(c in row for c in (\"ModernContig\",\"ModernStart\",\"ModernEnd\")):\n",
    "            s = row[\"ModernStart\"] + (1 if one_based else 0)\n",
    "            e = row[\"ModernEnd\"]\n",
    "            modern_str = f\"{row['ModernContig']}:{s}-{e}\"\n",
    "            modern_lines.append(modern_str)\n",
    "\n",
    "        coords.append((anc_str, modern_str))\n",
    "\n",
    "    anc_file = None\n",
    "    if anc_lines:\n",
    "        Path(anc_path).write_text(\"\\n\".join(anc_lines) + \"\\n\")\n",
    "\n",
    "    modern_file = None\n",
    "    if modern_lines:\n",
    "        Path(modern_path).write_text(\"\\n\".join(modern_lines) + \"\\n\")\n",
    "\n",
    "    return anc_path if anc_lines else None, modern_path if modern_lines else None, coords, strands\n",
    "\n",
    "\n",
    "import subprocess, signal\n",
    "from pathlib import Path\n",
    "\n",
    "def run_pipe_to_file(cmd1, cmd2, out_path):\n",
    "    \"\"\"Run: cmd1 | cmd2 > out_path, but report bcftools errors first.\"\"\"\n",
    "    Path(out_path).parent.mkdir(parents=True, exist_ok=True)\n",
    "    with open(out_path, \"wb\") as fout:\n",
    "        p1 = subprocess.Popen(cmd1, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "        p2 = subprocess.Popen(cmd2, stdin=p1.stdout, stdout=fout, stderr=subprocess.PIPE)\n",
    "        # Let p1 get SIGPIPE if p2 exits early\n",
    "        if p1.stdout is not None:\n",
    "            p1.stdout.close()\n",
    "\n",
    "        # Wait; read stderr streams\n",
    "        _, err2 = p2.communicate()\n",
    "        _, err1 = p1.communicate()\n",
    "\n",
    "        # 1) If bcftools failed, show that error (this is the usual root cause).\n",
    "        if p2.returncode != 0:\n",
    "            raise RuntimeError(\n",
    "                f\"{cmd2[0]} failed (exit {p2.returncode}).\\n{err2.decode(errors='ignore')}\"\n",
    "            )\n",
    "\n",
    "        # 2) If bcftools succeeded, ignore SIGPIPE (-13) from samtools; otherwise report samtools error.\n",
    "        if p1.returncode not in (0, -getattr(signal, \"SIGPIPE\", 13)):\n",
    "            raise RuntimeError(\n",
    "                f\"{cmd1[0]} failed (exit {p1.returncode}).\\n{err1.decode(errors='ignore')}\"\n",
    "            )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90db4384",
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "def run_adna_plus_modern(\n",
    "    set_name: str,\n",
    "    regions_df,\n",
    "    adna_label: str,            # e.g., \"richdalei\"\n",
    "    adna_sample: str,           # sample name in the aDNA BCF to use (e.g., \"filtered2\" or actual)\n",
    "    adna_bcf: str,              # e.g., \"richdalei_norm.bcf\"\n",
    "    anc_ref_fa: str,            # e.g., \"Anc01.fasta\"\n",
    "    modern_ref_fa: str,         # e.g., \"../a9_genome_masked.fa\"\n",
    "    modern_vcf: str,            # your modern population VCF/BCF\n",
    "    samples,          # modern samples list (one per line)\n",
    "    max_workers: int = 8,\n",
    "    region_start: int = 0,\n",
    "):\n",
    "    out_dir = Path(f\"out/{set_name}\")\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # 1) Region lists (1‑based inclusive)\n",
    "    anc_list, modern_list, coords, strands = write_regions_files(\n",
    "        regions_df,\n",
    "        anc_path=out_dir / \"anc_regions.txt\",\n",
    "        modern_path=out_dir / \"modern_regions.txt\",\n",
    "        one_based=True,\n",
    "    )\n",
    "    assert anc_list and modern_list, \"richdalei set should have both anc and modern coords\"\n",
    "\n",
    "    # Persist metadata for collation\n",
    "    json.dump(coords, open(out_dir / \"coords.json\", \"w\"))\n",
    "    json.dump(strands, open(out_dir / \"strand_flags.json\", \"w\"))\n",
    "\n",
    "    # 2) aDNA consensus (single run over all anc regions)\n",
    "    adna_multi = out_dir / f\"{adna_label}.multi.fa\"\n",
    "    cmd1 = [\"samtools\", \"faidx\", \"-r\", str(anc_list), anc_ref_fa]\n",
    "    cmd2 = [\"bcftools\", \"consensus\", \"-s\", adna_sample, \"-H\", \"I\", \"-M\", \"N\", adna_bcf]\n",
    "    run_pipe_to_file(cmd1, cmd2, adna_multi)\n",
    "\n",
    "    def do_one(sample):\n",
    "        outp = out_dir / f\"{sample}.multi.fa\"\n",
    "        c1 = [\"samtools\", \"faidx\", \"-r\", str(modern_list), modern_ref_fa]\n",
    "        c2 = [\"bcftools\", \"consensus\", \"-s\", sample, \"-H\", \"I\", \"-M\", \"N\", modern_vcf]\n",
    "        run_pipe_to_file(c1, c2, outp)\n",
    "        return str(outp)\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as ex:\n",
    "        futs = {ex.submit(do_one, s): s for s in samples}\n",
    "        for fut in as_completed(futs):\n",
    "            fut.result()  # raise on error\n",
    "\n",
    "    # 4) Collate per‑region folders; RC only the aDNA when '+-'\n",
    "    waitaha_like_collate(\n",
    "        set_name=set_name,\n",
    "        adna_label=adna_label,\n",
    "        out_dir=str(out_dir),\n",
    "        rc_strands=True,  # enable RC per '+-' flag\n",
    "        region_start=region_start,  # start from 0 or 1 as needed\n",
    "    )\n",
    "\n",
    "\n",
    "def waitaha_like_collate(set_name: str, adna_label: str, out_dir: str, rc_strands: bool, region_start: int = 0):\n",
    "    out_dir = Path(out_dir); \n",
    "    coords = json.load(open(out_dir / \"coords.json\"))\n",
    "    strands = json.load(open(out_dir / \"strand_flags.json\")) if rc_strands else None\n",
    "\n",
    "    # Load aDNA records\n",
    "    adna_records = list(parse_fastx_file(out_dir / f\"{adna_label}.multi.fa\"))\n",
    "\n",
    "    # Load modern records per sample\n",
    "    modern_records = {}\n",
    "    for p in glob.glob(str(out_dir / \"*.multi.fa\")):\n",
    "        name = Path(p).stem.replace(\".multi\", \"\")\n",
    "        if name == adna_label:\n",
    "            continue\n",
    "        modern_records[name] = list(parse_fastx_file(p))\n",
    "\n",
    "    n = len(adna_records)\n",
    "    assert n == len(coords), \"Count mismatch (coords vs aDNA records)\"\n",
    "    for s, recs in modern_records.items():\n",
    "        assert len(recs) == n, f\"Modern count mismatch for {s}\"\n",
    "\n",
    "    for i, ((anc_str, modern_str)) in enumerate(coords):\n",
    "        d = f\"regions/region_{i + region_start}\"\n",
    "        d = Path(d)\n",
    "        d.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        # coordinates.txt\n",
    "        with open(d / \"coordinates.txt\", \"w\") as f:\n",
    "            if anc_str:   f.write(anc_str + \"\\n\")\n",
    "            if modern_str:f.write(modern_str + \"\\n\")\n",
    "\n",
    "        # aDNA file (header >{adna_label}); RC when '+-'\n",
    "        arec = adna_records[i]\n",
    "        seq = arec.seq\n",
    "        if strands:\n",
    "            flag = strands[i]\n",
    "            if flag == \"+-\":\n",
    "                seq = reverse_complement(seq)\n",
    "            elif flag != \"++\":\n",
    "                print(f\"[{set_name}] Unexpected strand {flag} at region_{i}; skipping\")\n",
    "                continue\n",
    "\n",
    "        with open(d / f\"{adna_label}.fasta\", \"w\") as f:\n",
    "            adna_label = sample_id_map.get(adna_label, adna_label)\n",
    "            f.write(f\">{adna_label}\\n{seq}\\n\")\n",
    "\n",
    "        # Modern per sample (never RC)\n",
    "        for s, recs in modern_records.items():\n",
    "            mseq = recs[i].seq\n",
    "            # Get sample from sample_id_map\n",
    "            sample = sample_id_map.get(s, s)  # Fallback to original if not\n",
    "            with open(d / f\"{s}.fasta\", \"w\") as f:\n",
    "                f.write(f\">{s}\\n{mseq}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32c5d6cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_adna_plus_modern(\n",
    "    set_name=\"richdalei\",\n",
    "    regions_df=richdalei_regions,      # your DataFrame\n",
    "    adna_label=\"richdalei\",\n",
    "    adna_sample=\"with_rg\",           # or the exact sample name in the BCF\n",
    "    adna_bcf=\"richdalei_norm.bcf\",\n",
    "    anc_ref_fa=\"Anc01.fasta\",\n",
    "    modern_ref_fa=\"../a9_genome_masked.fa\",\n",
    "    modern_vcf=vcf_file,               # your modern pop VCF/BCF\n",
    "    samples=samples,\n",
    "    max_workers=8,\n",
    "    region_start=len(waitaha_regions)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93e2c19f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pixi run python realign_regions.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4e9ab23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This should make them all the same length as modern chromosome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24af489d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_modern_only(\n",
    "    set_name: str,\n",
    "    regions_df,\n",
    "    modern_ref_fa: str,\n",
    "    modern_vcf: str,\n",
    "    samples,\n",
    "    max_workers: int = 8,\n",
    "    region_start: int = 0,\n",
    "    bin_size: int = 5000,  # 5kb bins\n",
    "):\n",
    "    out_dir = Path(f\"out/{set_name}\")\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    reg_dir = Path(f\"regions/\")\n",
    "    reg_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # In columns are \"CHROM\" and \"BIN_START\". We can get the end with \"BIN_START + BIN_SIZE\"\n",
    "    # Need to convert to the format for the other tools\n",
    "\n",
    "    # We need regions_df to be: \"ModernContig\",\"ModernStart\",\"ModernEnd\", but ours is the CHROM, BIN_START, BIN_SIZE\n",
    "    regions_df = regions_df.select([\n",
    "        pl.col(\"CHROM\").alias(\"ModernContig\"),\n",
    "        (pl.col(\"BIN_START\")).alias(\"ModernStart\"),  # convert to 1-based inclusive\n",
    "        (pl.col(\"BIN_START\") + bin_size).alias(\"ModernEnd\")\n",
    "    ])\n",
    "\n",
    "    # Only modern regions are required; anc columns may be absent\n",
    "    anc_list, modern_list, coords, _ = write_regions_files(\n",
    "        regions_df,\n",
    "        anc_path=out_dir / \"anc_regions.txt\",       # may not be used\n",
    "        modern_path=out_dir / \"modern_regions.txt\",\n",
    "        one_based=True,\n",
    "    )\n",
    "    assert modern_list, \"modern_regions set must have modern coordinates\"\n",
    "    json.dump(coords, open(out_dir / \"coords.json\", \"w\"))\n",
    "\n",
    "    def do_one(sample):\n",
    "        outp = out_dir / f\"{sample}.multi.fa\"\n",
    "        c1 = [\"samtools\", \"faidx\", \"-r\", str(modern_list), modern_ref_fa]\n",
    "        c2 = [\"bcftools\", \"consensus\", \"-s\", sample, \"-H\", \"I\", \"-M\", \"N\", modern_vcf]\n",
    "\n",
    "        run_pipe_to_file(c1, c2, outp)\n",
    "        return str(outp)\n",
    "\n",
    "    from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as ex:\n",
    "        futs = {ex.submit(do_one, s): s for s in samples}\n",
    "        for fut in as_completed(futs):\n",
    "            fut.result()\n",
    "\n",
    "    # Collate: per‑region folders with coordinates.txt and {sample}.fasta files\n",
    "    coords = json.load(open(out_dir / \"coords.json\"))\n",
    "    # Load modern records per sample\n",
    "    modern_records = {Path(p).stem.replace(\".multi\",\"\"): list(parse_fastx_file(p))\n",
    "                      for p in glob.glob(str(out_dir / \"*.multi.fa\"))}\n",
    "\n",
    "    n = len(next(iter(modern_records.values())))\n",
    "    assert n == len(coords), \"Count mismatch (coords vs first sample records)\"\n",
    "    for s, recs in modern_records.items():\n",
    "        assert len(recs) == n, f\"Region count mismatch for sample {s}\"\n",
    "\n",
    "    for i, (anc_str, modern_str) in enumerate(coords):\n",
    "        d = reg_dir / f\"region_{i + region_start}\"\n",
    "        d.mkdir(parents=True, exist_ok=True)\n",
    "        with open(d / \"coordinates.txt\", \"w\") as f:\n",
    "            if anc_str:   f.write(anc_str + \"\\n\")    # present if df had ancient cols\n",
    "            if modern_str:f.write(modern_str + \"\\n\")\n",
    "\n",
    "        for s, recs in modern_records.items():\n",
    "            mseq = recs[i].seq\n",
    "            with open(d / f\"{s}.fasta\", \"w\") as f:\n",
    "                sample = sample_id_map.get(s, s)\n",
    "                f.write(f\">{s}\\n{mseq}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a4d9bbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_other\n",
    "n_other = 200\n",
    "modern_only_regions = snpden_filtered.sample(n_other, with_replacement=False)\n",
    "run_modern_only(\n",
    "    set_name=\"modern_only\",\n",
    "    regions_df=modern_only_regions,\n",
    "    modern_ref_fa=\"../a9_genome_masked.fa\",\n",
    "    modern_vcf=vcf_file,               # your modern pop VCF/BCF\n",
    "    samples=samples,\n",
    "    max_workers=8,\n",
    "    region_start=500,\n",
    "    bin_size=2000 # 2kbp\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c77c3fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "vcf = VCF(vcf_file)\n",
    "a9_samples = vcf.samples\n",
    "\n",
    "HALSTATS_PATH = \"../seabird_alignment_halstats\"\n",
    "\n",
    "# aDNA Samples\n",
    "adna_samples = [\"waitaha\", \"richdalei\"]\n",
    "\n",
    "# Seabird Samples from halstats and Cactus\n",
    "seabirds_df = pl.read_csv(HALSTATS_PATH, skip_lines=4)['GenomeName'].to_list()\n",
    "seabirds_df = [s for s in seabirds_df if s is not None]\n",
    "seabirds_df = [s for s in seabirds_df if not s.startswith(\"Anc\")]\n",
    "# Filter out c90 (it's subantarctic islands and we have them from the regular pop)\n",
    "seabirds_df = [s for s in seabirds_df if not s.startswith(\"c90\")]\n",
    "seabirds_df = [s for s in seabirds_df if not s.startswith(\"a9\")] # We have a9 in the SNPs as well\n",
    "# Remove Megadyptes_antipodes\n",
    "seabirds_df = [s for s in seabirds_df if not s.startswith(\"Megadyptesantipodes\")]\n",
    "# Filter out samples as in the notebook\n",
    "# all_seabirds = [s for s in all_seabirds if not s.startswith((\"c90\", \"a9\", \"Megadyptesantipodes\"))]\n",
    "all_seabirds = seabirds_df\n",
    "\n",
    "# Penguin Samples (subset of seabirds)\n",
    "penguin_prefixes = (\"Aptenodytes\", \"Spheniscus\", \"Pygoscelis\", \"Eudyptula\", \"Eudyptes\")\n",
    "penguins = [sp for sp in all_seabirds if sp.startswith(penguin_prefixes)]\n",
    "\n",
    "# --- Define the sets of samples to analyze ---\n",
    "cactus_penguins = penguins\n",
    "cactus_seabirds = all_seabirds\n",
    "\n",
    "default_outgroups = [\"Eudyptesmoseleyi_genomic\", \"Spheniscushumboldti_genomic\", \"Eudyptesfilholi_genomic\"]\n",
    "\n",
    "TARGET_SAMPLE_SETS = {\n",
    "    \"penguins\": a9_samples + adna_samples + cactus_penguins,\n",
    "    \"all_seabirds\": a9_samples + adna_samples + cactus_seabirds,\n",
    "    \"vcf_and_adna_only\": a9_samples + adna_samples,\n",
    "    \"just_a_few_outgroups\": a9_samples + adna_samples + default_outgroups\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "640665e9",
   "metadata": {},
   "source": [
    "## Add in the other species"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "255905cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix:         RuntimeError: invalid range start=25620000 end=25622000 exceeds ptg000013l sequence length of 25621374\n",
    "# ../a9_genome_masked.fa.fai\n",
    "# Read in the fai file\n",
    "\n",
    "fai_file = \"../a9_genome_masked.fa.fai\"\n",
    "fai = pl.read_csv(fai_file, separator=\"\\t\", has_header=False, new_columns=[\"Contig\", \"Length\", \"Offset\", \"LineBases\", \"LineWidth\"])\n",
    "fai = fai.with_columns(pl.col(\"Length\").cast(pl.Int64))\n",
    "# Create a dictionary for quick access\n",
    "fai_dict = {row[\"Contig\"]: row[\"Length\"] for row in fai.iter_rows(named=True)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58e2b1a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Run cactus-hal2maf and then oxid_maf remove-ref-indels for each region in parallel.\n",
    "\n",
    "Requirements from user:\n",
    "- Jobstore: unique per run & region, MUST NOT exist beforehand, and NOT under /tmp.\n",
    "- WorkDir: unique per run & region, MUST EXIST beforehand, on the same drive as before (/mnt/data).\n",
    "\n",
    "Other features:\n",
    "- Up to MAX_WORKERS regions in flight.\n",
    "- Per-region MAF output (region/hal2maf.maf).\n",
    "- Renames resulting FASTA headers using sample_id_map.\n",
    "- Adjusts extraction length if it would exceed contig length (fai_dict).\n",
    "\"\"\"\n",
    "\n",
    "from glob import glob\n",
    "import subprocess\n",
    "import os\n",
    "import sys\n",
    "import uuid\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "# ------------ config ------------\n",
    "MAX_WORKERS = 6\n",
    "\n",
    "HAL_PATH = \"/mnt/data/seabirds.hal\"\n",
    "CACTUS_BIN = \"/mnt/data/development/hoiho/wga/cactus/cactus-bin-v2.9.7\"\n",
    "CACTUS_VENV = \"/mnt/data/development/hoiho/wga/cactus/cactus-bin-v2.9.7/venv-cactus-v2.9.7\"\n",
    "CACTUS_HAL2MAF = f\"{CACTUS_VENV}/bin/cactus-hal2maf\"\n",
    "\n",
    "# Bases (same drive: /mnt/data)\n",
    "BASE_JOBSTORE = Path(\"/mnt/data/hal2maf_jobstores\")    # jobstores live here; per-run subdirs\n",
    "BASE_WORKDIR  = Path(\"/mnt/data/workdir_hal2maf\")      # work dirs live here; per-run subdirs\n",
    "\n",
    "# oxid_maf path & args\n",
    "OXID_MAF = \"/home/joseph/development/OxidMAF/target/release/oxid_maf\"\n",
    "OXID_SUBCOMMAND = \"remove-ref-indels\"\n",
    "OXID_EXCLUDE = \"c90,a9,Megadyptesantipodesantipodes_genomic\"  # from modern pop\n",
    "\n",
    "# Environment (ensure cactus bin/venv are in PATH)\n",
    "subprocess_env = os.environ.copy()\n",
    "subprocess_env[\"PATH\"] = f\"{CACTUS_BIN}/bin:{CACTUS_VENV}/bin:\" + subprocess_env.get(\"PATH\", \"\")\n",
    "\n",
    "# Create a unique RUN_ID for this invocation\n",
    "RUN_ID = datetime.now().strftime(\"%Y%m%d-%H%M%S\") + \"-\" + uuid.uuid4().hex[:8]\n",
    "\n",
    "# ------------ helpers ------------\n",
    "\n",
    "def parse_ptg_coords(coords_path: Path):\n",
    "    \"\"\"\n",
    "    Parse coordinates.txt and return (contig, start0, length).\n",
    "    Expects a line like: ptg000001l:42594242-42599241  (1-based, inclusive end).\n",
    "    We convert to 0-based start and compute length.\n",
    "    \"\"\"\n",
    "    with coords_path.open() as f:\n",
    "        lines = [ln.strip() for ln in f if ln.strip()]\n",
    "    ptg_lines = [ln for ln in lines if ln.startswith(\"ptg\")]\n",
    "    if not ptg_lines:\n",
    "        return None\n",
    "\n",
    "    contig, span = ptg_lines[0].split(\":\")\n",
    "    start_s, end_s = span.split(\"-\")\n",
    "    start0 = int(start_s) - 1\n",
    "    end1  = int(end_s)      # inclusive end in 1-based input; here we use end (0-based exclusive) logic via length\n",
    "    length = end1 - start0\n",
    "    if length <= 0:\n",
    "        raise ValueError(f\"Non-positive length parsed from {coords_path}: {ptg_lines[0]}\")\n",
    "    return contig, start0, length\n",
    "\n",
    "\n",
    "def process_one_region(region: Path, fai_dict, sample_id_map):\n",
    "    \"\"\"\n",
    "    Process a single region directory with a per-run unique jobstore and workdir.\n",
    "\n",
    "    Jobstore rules:\n",
    "      - jobstore path must NOT exist before starting.\n",
    "      - we do not create it; cactus-hal2maf will.\n",
    "\n",
    "    WorkDir rules:\n",
    "      - workdir path must EXIST before starting.\n",
    "      - we create it; fail if it already exists (violates uniqueness).\n",
    "    \"\"\"\n",
    "    try:\n",
    "        coords_path = region / \"coordinates.txt\"\n",
    "        if not coords_path.exists():\n",
    "            return (region, False, \"coordinates.txt not found\")\n",
    "\n",
    "        parsed = parse_ptg_coords(coords_path)\n",
    "        if not parsed:\n",
    "            return (region, False, \"No ptg* coordinates found\")\n",
    "\n",
    "        contig, start0, length = parsed\n",
    "\n",
    "        # Bounds adjust using fai_dict\n",
    "        contig_len = fai_dict.get(contig)\n",
    "        if contig_len is None:\n",
    "            return (region, False, f\"Contig {contig} not found in fai_dict\")\n",
    "        if start0 >= contig_len:\n",
    "            return (region, False, f\"Start {start0} beyond contig length {contig_len}\")\n",
    "        if start0 + length > contig_len:\n",
    "            length = contig_len - start0\n",
    "\n",
    "        # Per-run, per-region paths\n",
    "        region_name = region.name\n",
    "        jobstore = BASE_JOBSTORE / RUN_ID / region_name      # MUST NOT exist\n",
    "        workdir  = BASE_WORKDIR  / RUN_ID / region_name      # MUST exist\n",
    "\n",
    "        # Ensure parents exist\n",
    "        jobstore.parent.mkdir(parents=True, exist_ok=True)\n",
    "        workdir.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        # Enforce jobstore non-existence\n",
    "        if jobstore.exists():\n",
    "            return (region, False, f\"Jobstore already exists (must not): {jobstore}\")\n",
    "\n",
    "        # Enforce workdir existence (unique per run, so it should not exist yet)\n",
    "        if workdir.exists():\n",
    "            return (region, False, f\"WorkDir already exists (violates uniqueness): {workdir}\")\n",
    "        workdir.mkdir(parents=False, exist_ok=False)  # create; must exist before run\n",
    "\n",
    "        maf_output = region / \"hal2maf.maf\"  # per-region MAF; overwrite each run\n",
    "        if maf_output.exists():\n",
    "            maf_output.unlink()\n",
    "\n",
    "        print(f\"▶ {region_name}: {contig}:{start0}-{start0+length} ({length}bp)\")\n",
    "        print(f\"   jobstore={jobstore}\")\n",
    "        print(f\"   workdir={workdir}\")\n",
    "\n",
    "        # cactus-hal2maf\n",
    "        cmd_hal = [\n",
    "            CACTUS_HAL2MAF,\n",
    "            str(jobstore),\n",
    "            HAL_PATH,\n",
    "            str(maf_output),\n",
    "            \"--noAncestors\",\n",
    "            \"--clean\", \"always\",\n",
    "            \"--workDir\", str(workdir),\n",
    "            \"--chunkSize\", \"20000\",\n",
    "            \"--binariesMode\", \"local\",\n",
    "            \"--dupeMode\", \"single\",\n",
    "            \"--refSequence\", contig,\n",
    "            \"--refGenome\", \"a9\",\n",
    "            \"--start\", str(start0),\n",
    "            \"--length\", str(length),\n",
    "        ]\n",
    "        res = subprocess.run(cmd_hal, capture_output=True, text=True, env=subprocess_env)\n",
    "        if res.returncode != 0:\n",
    "            return (region, False, f\"cactus-hal2maf failed:\\nSTDERR:\\n{res.stderr}\\nSTDOUT:\\n{res.stdout}\")\n",
    "\n",
    "        # oxid_maf: remove ref indels\n",
    "        cmd_oxid = [\n",
    "            OXID_MAF,\n",
    "            OXID_SUBCOMMAND,\n",
    "            str(maf_output),\n",
    "            str(region) + \"/\",   # output dir\n",
    "            \"--exclude\",\n",
    "            OXID_EXCLUDE,\n",
    "        ]\n",
    "        res = subprocess.run(cmd_oxid, capture_output=True, text=True, env=subprocess_env)\n",
    "        if res.returncode != 0:\n",
    "            return (region, False, f\"oxid_maf failed:\\nSTDERR:\\n{res.stderr}\\nSTDOUT:\\n{res.stdout}\")\n",
    "\n",
    "        # Rename headers in emitted FASTAs using sample_id_map\n",
    "        for fasta_path in region.glob(\"*.fasta\"):\n",
    "            with fasta_path.open() as fh:\n",
    "                lines = fh.readlines()\n",
    "            if not lines:\n",
    "                continue\n",
    "            header = lines[0].strip()\n",
    "            seq = \"\".join(lines[1:]).strip()\n",
    "            sample_name = header[1:] if header.startswith(\">\") else header\n",
    "            new_name = sample_id_map.get(sample_name, sample_name)\n",
    "            if new_name != sample_name:\n",
    "                with fasta_path.open(\"w\") as out:\n",
    "                    out.write(f\">{new_name}\\n{seq}\\n\")\n",
    "\n",
    "        # Success; do NOT delete jobstore/workdir automatically (kept for provenance/resume)\n",
    "        return (region, True, \"ok\")\n",
    "\n",
    "    except Exception as e:\n",
    "        return (region, False, f\"Exception: {e}\")\n",
    "\n",
    "\n",
    "# ------------ main ------------\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Expect fai_dict and sample_id_map to be defined in the environment where this script is run\n",
    "    try:\n",
    "        fai_dict\n",
    "        sample_id_map\n",
    "    except NameError:\n",
    "        print(\"ERROR: 'fai_dict' and 'sample_id_map' must be defined in scope before running this script.\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    regions = [Path(p) for p in sorted(glob(\"regions/region_*\")) if Path(p).is_dir()]\n",
    "    if not regions:\n",
    "        print(\"No regions found.\")\n",
    "        sys.exit(0)\n",
    "\n",
    "    print(f\"RUN_ID={RUN_ID}\")\n",
    "    print(f\"Submitting {len(regions)} regions with up to {MAX_WORKERS} concurrent jobs…\")\n",
    "    failures = []\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=MAX_WORKERS) as ex:\n",
    "        futs = {ex.submit(process_one_region, r, fai_dict, sample_id_map): r for r in regions}\n",
    "        for fut in as_completed(futs):\n",
    "            region = futs[fut]\n",
    "            ok_region, ok, msg = fut.result()\n",
    "            name = ok_region.name\n",
    "            if ok:\n",
    "                print(f\"✓ {name}: {msg}\")\n",
    "            else:\n",
    "                print(f\"✗ {name}: {msg}\")\n",
    "                failures.append((name, msg))\n",
    "\n",
    "    if failures:\n",
    "        print(\"\\nSummary of failures:\")\n",
    "        for name, msg in failures:\n",
    "            print(f\" - {name}: {msg}\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    print(\"\\nAll regions completed successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "933e4b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's save the waitaha_regions, richdalei_regions, and modern_only_regions to a file\n",
    "#waitaha_regions.write_parquet(\"waitaha_regions.parquet\")\n",
    "#richdalei_regions.write_parquet(\"richdalei_regions.parquet\")\n",
    "#modern_only_regions.write_parquet(\"modern_only_regions.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec0bdce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "vcf = VCF(vcf_file)\n",
    "a9_samples = vcf.samples\n",
    "\n",
    "HALSTATS_PATH = \"../seabird_alignment_halstats\"\n",
    "\n",
    "# aDNA Samples\n",
    "adna_samples = [\"waitaha\", \"richdalei\"]\n",
    "\n",
    "# Seabird Samples from halstats and Cactus\n",
    "seabirds_df = pl.read_csv(HALSTATS_PATH, skip_lines=4)['GenomeName'].to_list()\n",
    "seabirds_df = [s for s in seabirds_df if s is not None]\n",
    "seabirds_df = [s for s in seabirds_df if not s.startswith(\"Anc\")]\n",
    "# Filter out c90 (it's subantarctic islands and we have them from the regular pop)\n",
    "seabirds_df = [s for s in seabirds_df if not s.startswith(\"c90\")]\n",
    "seabirds_df = [s for s in seabirds_df if not s.startswith(\"a9\")] # We have a9 in the SNPs as well\n",
    "# Remove Megadyptes_antipodes\n",
    "seabirds_df = [s for s in seabirds_df if not s.startswith(\"Megadyptesantipodes\")]\n",
    "# Filter out samples as in the notebook\n",
    "# all_seabirds = [s for s in all_seabirds if not s.startswith((\"c90\", \"a9\", \"Megadyptesantipodes\"))]\n",
    "all_seabirds = seabirds_df\n",
    "\n",
    "# Penguin Samples (subset of seabirds)\n",
    "penguin_prefixes = (\"Aptenodytes\", \"Spheniscus\", \"Pygoscelis\", \"Eudyptula\", \"Eudyptes\")\n",
    "penguins = [sp for sp in all_seabirds if sp.startswith(penguin_prefixes)]\n",
    "\n",
    "# --- Define the sets of samples to analyze ---\n",
    "cactus_penguins = penguins\n",
    "cactus_seabirds = all_seabirds\n",
    "\n",
    "default_outgroups = [\"Eudyptesmoseleyi_genomic\", \"Spheniscushumboldti_genomic\", \"Eudyptesfilholi_genomic\"]\n",
    "\n",
    "TARGET_SAMPLE_SETS = {\n",
    "    \"penguins\": a9_samples + adna_samples + cactus_penguins,\n",
    "    \"all_seabirds\": a9_samples + adna_samples + cactus_seabirds,\n",
    "    \"vcf_and_adna_only\": a9_samples + adna_samples,\n",
    "    \"just_a_few_outgroups\": a9_samples + adna_samples + default_outgroups\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e9bde7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each region\n",
    "region_dirs = glob(\"regions/region_*\")\n",
    "\n",
    "for sample_set, samples in TARGET_SAMPLE_SETS.items():\n",
    "    # Create the output directory this this sample set \"processed/{sample_set}\"\n",
    "    out_dir = Path(f\"processed/{sample_set}\")\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    for region in region_dirs:\n",
    "        # Create a blank {region_name}.fasta file in the out_dir\n",
    "        region_name = Path(region).name\n",
    "        with open(out_dir / f\"{region_name}.fasta\", \"w\") as fh:\n",
    "            print(f\"Processing {region_name} for sample set {sample_set}\")\n",
    "            \n",
    "            for sample in samples:\n",
    "                # Check for sample.fasta or mapped sample id.fasta\n",
    "                sample_fasta = Path(region) / f\"{sample}.fasta\"\n",
    "                if not sample_fasta.exists():\n",
    "                    # Try mapped sample id\n",
    "                    mapped_sample = sample_id_map.get(sample, sample)\n",
    "                    sample_fasta = Path(region) / f\"{mapped_sample}.fasta\"\n",
    "\n",
    "                if not sample_fasta.exists():\n",
    "                    continue\n",
    "\n",
    "                # Read the sample FASTA\n",
    "                with open(sample_fasta, \"r\") as sfh:\n",
    "                    lines = sfh.readlines()\n",
    "                    if not lines:\n",
    "                        continue\n",
    "\n",
    "                    # Write the header and sequence to the output file\n",
    "                    header = lines[0].strip()\n",
    "                    seq = \"\".join(lines[1:]).strip()\n",
    "                    fh.write(f\"{header}\\n{seq}\\n\")\n",
    "        # Finally, run pixi run mafft on the output file (then rename to the original region fasta file, delete intermediate)\n",
    "        out_fasta = out_dir / f\"{region_name}.fasta\"\n",
    "        # Define the path for the aligned output FASTA file\n",
    "        aligned_fasta = out_dir / f\"{region_name}.aligned.fasta\"\n",
    "\n",
    "        # Correctly run mafft by redirecting standard output to the aligned file\n",
    "        with open(aligned_fasta, \"w\") as output_file:\n",
    "            cmd_mafft = [\"pixi\", \"run\", \"mafft\", \"--quiet\", str(out_fasta)]\n",
    "            subprocess.run(cmd_mafft, check=True, stdout=output_file, stderr=subprocess.DEVNULL)\n",
    "\n",
    "        # Rename the aligned file to replace the original unaligned file\n",
    "        # The original 'aligned_fasta' file is renamed, so no separate deletion is needed.\n",
    "        aligned_fasta.rename(out_fasta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2654e27",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d588b593",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix any indels that snuck in\n",
    "\n",
    "sample_id_map_values = [sample_id_map[s] for s in a9_samples + adna_samples]\n",
    "\n",
    "processed_files = glob(\"processed/*/*.fasta\")\n",
    "for processed_file in processed_files:\n",
    "    lengths = {}\n",
    "    with open(processed_file, \"r\") as f:\n",
    "        lines = f.readlines()\n",
    "    if not lines:\n",
    "        continue\n",
    "    \n",
    "    # Check that all sequence lengths are the same (for modern pop and aDNA samples)\n",
    "    seqs = []\n",
    "    # Take 2 lines (id and seq), make sure id is in sample_id_map (values), then record the length\n",
    "    for i in range(0, len(lines), 2):\n",
    "        if i + 1 >= len(lines):\n",
    "            continue  # skip if no sequence line\n",
    "        header = lines[i].strip()\n",
    "        seq = lines[i + 1].strip()\n",
    "        if header[1:] not in sample_id_map_values:  # skip if not in sample_id_map\n",
    "            continue\n",
    "        lengths[header[1:]] = len(seq)\n",
    "\n",
    "    if len(set(lengths.values())) > 1:\n",
    "        print(f\"Error in {processed_file}: sequences have different lengths: {set(lengths.values())}\")\n",
    "        break\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12837130",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "default",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
