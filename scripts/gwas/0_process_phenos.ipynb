{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f2bd36c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import plotly.express as px\n",
    "from scipy.stats import chi2\n",
    "import numpy as np  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "520198b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"plink_roh.hom.indiv\", \"r\") as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "# Trim excess spaces (collapse multiple ones, and remove leading/trailing)\n",
    "cleaned_lines = [' '.join(line.split()) for line in lines]\n",
    "\n",
    "# Create dataframe\n",
    "data = [line.split(' ') for line in cleaned_lines]\n",
    "\n",
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0873458d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ['FID', 'IID', 'PHE', 'NSEG', 'KB', 'KBAVG']\n",
    "\n",
    "# datadict, Sequence, ndarray, Series, or pandas.DataFrame\n",
    "#   Two-dimensional data in various forms; dict input must contain Sequences, Generators, or a range. Sequence may contain Series or other Sequences.\n",
    "# Convert to dict\n",
    "columns = [\"FID\", \"IID\", \"PHE\", \"NSEG\", \"KB\", \"KBAVG\"]\n",
    "data_dict = {col: [] for col in columns}\n",
    "\n",
    "for row in data[1:]:\n",
    "    for col, value in zip(columns, row):\n",
    "        data_dict[col].append(value)\n",
    "\n",
    "plink_roh_df = pl.DataFrame(data_dict)\n",
    "plink_roh_df.head()\n",
    "# Keep only FID, and KB. Then make a new column that is KB / 1,360,000 called FROH\n",
    "plink_roh_df = plink_roh_df.select([\n",
    "    pl.col(\"FID\"),\n",
    "    (pl.col(\"KB\").cast(pl.Float64) / 1_360_000).alias(\"FROH\")\n",
    "])\n",
    "plink_roh_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7300640",
   "metadata": {},
   "outputs": [],
   "source": [
    "phenos_df = pl.read_csv(\"../Hoiho_Genomes_Cleaned_Sep11_25.csv\", separator=\"\\t\")\n",
    "phenos_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d3aadfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count \"RDS Status\" values\n",
    "phenos_df.group_by(\"RDS Status\").agg(pl.count()).sort(\"RDS Status\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a5e9001",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many in just the northern pop?\n",
    "phenos_df.filter(pl.col(\"Population (new)\") == \"Northern \").group_by(\"RDS Status\").agg(pl.count()).sort(\"RDS Status\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b45ae4f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using polars, let's build a new dataframe for the phenotypes and covariates\n",
    "# Let's start with the covariates as they will be the most 'exhaustive'\n",
    "\n",
    "# phenos_df['Population (new)'].unique()\n",
    "# \"Northern \"\n",
    "# \"Campbell\"\n",
    "# \"Enderby\"\n",
    "\n",
    "# We need to go from population new column to dummy variables\n",
    "# is_northern, is_campbell (is_enderby is the reference)\n",
    "\n",
    "covars_df = phenos_df.with_columns([\n",
    "    (pl.col(\"Population (new)\") == \"Northern \").cast(pl.Int8).alias(\"is_northern\"),\n",
    "    (pl.col(\"Population (new)\") == \"Campbell\").cast(pl.Int8).alias(\"is_campbell\")])\n",
    "\n",
    "# Then is_male (female as ref)\n",
    "covars_df = covars_df.with_columns([\n",
    "    (pl.col(\"Sex (genetics)\") == \"Male\").cast(pl.Int8).alias(\"is_male\")\n",
    "])\n",
    "\n",
    "# Then season (S1, S2, S3, S4) but let's use S1 as ref\n",
    "covars_df = covars_df.with_columns([\n",
    "    (pl.col(\"Season\") == \"S2\").cast(pl.Int8).alias(\"is_s2\"),\n",
    "    (pl.col(\"Season\") == \"S3\").cast(pl.Int8).alias(\"is_s3\"),\n",
    "    (pl.col(\"Season\") == \"S4\").cast(pl.Int8).alias(\"is_s4\")\n",
    "])\n",
    "\n",
    "# Keep only the covariate columns (and ID, of course!)\n",
    "covars_df = covars_df.select([\n",
    "    \"ID\",\n",
    "    \"is_northern\",\n",
    "    \"is_campbell\",\n",
    "    \"is_male\",\n",
    "    \"is_s2\",\n",
    "    \"is_s3\",\n",
    "    \"is_s4\"\n",
    "])\n",
    "\n",
    "# Join with FROH\n",
    "covars_df = covars_df.join(plink_roh_df, left_on=\"ID\", right_on=\"FID\", how=\"left\")\n",
    "# Save to file, as TSV\n",
    "covars_df.write_csv(\"hoiho_covariates_15Sept2025.tsv\", separator=\"\\t\")\n",
    "\n",
    "covars_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f79c0f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop is_male and the seasons (s2, s3, s4)\n",
    "covars_df = covars_df.select([\n",
    "    \"ID\",\n",
    "    \"is_northern\",\n",
    "    \"is_campbell\",\n",
    "    \"FROH\"\n",
    "])\n",
    "covars_df.write_csv(\"hoiho_covariates_24Sept2025_islands_froh.tsv\", separator=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77e83b5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "phenos_df.head()\n",
    "\n",
    "# Let's convert the phenotypes to 1/0 (except RDS Severity, which we will keep as quantitative)\n",
    "# We will use the following mappings:\n",
    "# RDS Status: RDS -> 1, No -> 0, Unknown -> nan\n",
    "# DS Status: Y -> 1, N -> 0, null -> nan\n",
    "# 'RDS Severity Score (out of 10)' -> keep quantitative, replace null with nan\n",
    "# 'GV Status (P2 P3 primers)' Pos -> 1, Neg -> 0\n",
    "\n",
    "# This is not converting \"Unknown\" to nan\n",
    "phenos_df = phenos_df.with_columns([\n",
    "    pl.when(pl.col(\"RDS Status\") == \"RDS\")\n",
    "      .then(1)\n",
    "      .when(pl.col(\"RDS Status\") == \"No\")\n",
    "      .then(0)\n",
    "      .otherwise(None)\n",
    "      .cast(pl.Int8)\n",
    "      .alias(\"rds_status\"),\n",
    "\n",
    "    pl.when(pl.col(\"DS Status\") == \"Y\")\n",
    "      .then(1)\n",
    "      .when(pl.col(\"DS Status\") == \"N\")\n",
    "      .then(0)\n",
    "      .otherwise(None)\n",
    "      .cast(pl.Int8)\n",
    "      .alias(\"ds_status\"),\n",
    "\n",
    "    pl.col(\"RDS Severity Score (out of 10)\")\n",
    "      .cast(pl.Float32)\n",
    "      .alias(\"rds_severity\"),\n",
    "\n",
    "    pl.when(pl.col(\"GV Status (P2 P3 primers)\") == \"Pos\")\n",
    "      .then(1)\n",
    "      .when(pl.col(\"GV Status (P2 P3 primers)\") == \"Neg\")\n",
    "      .then(0)\n",
    "      .otherwise(None)\n",
    "      .cast(pl.Int8)\n",
    "      .alias(\"gv_status\"),\n",
    "])\n",
    "\n",
    "# Now only keep the ID and processed columns\n",
    "phenos_df = phenos_df.select([\n",
    "    \"ID\",\n",
    "    \"rds_status\",\n",
    "    \"ds_status\",\n",
    "    \"rds_severity\",\n",
    "    \"gv_status\"\n",
    "])\n",
    "\n",
    "phenos_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd604730",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's visualize rds_severity\n",
    "phenos_df_filtered = phenos_df.filter(pl.col(\"rds_severity\").is_not_null())\n",
    "fig = px.histogram(phenos_df_filtered.to_pandas(), x=\"rds_severity\", nbins=20, title=\"RDS Severity Distribution\")\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58891277",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import erfinv\n",
    "from scipy.stats import norm\n",
    "import pandas as pd\n",
    "\n",
    "def inverse_normal_transform_scipy(series: pd.Series) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Apply rank-based Inverse Normal Transformation to a pandas Series.\n",
    "    Handles ties by averaging ranks. Ensures NaNs are handled correctly.\n",
    "    \"\"\"\n",
    "    # Rank the data. Higher values get higher ranks.\n",
    "    # NaNs are kept as NaNs and don't affect the ranking of other values.\n",
    "    ranked = series.rank(method='average', na_option='keep')\n",
    "    \n",
    "    # Get the number of non-NA values\n",
    "    n = series.notna().sum()\n",
    "    \n",
    "    # Convert ranks to quantiles\n",
    "    quantiles = (ranked - 0.5) / n\n",
    "    \n",
    "    # Apply the inverse normal transformation (quantile function)\n",
    "    return norm.ppf(quantiles)\n",
    "\n",
    "# Apply the transformation\n",
    "phenos_df = phenos_df.with_columns(\n",
    "    pl.Series(\"rds_severity_int\", inverse_normal_transform_scipy(phenos_df[\"rds_severity\"].to_pandas()))\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fb74a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's visualize rds_severity\n",
    "phenos_df_filtered = phenos_df.filter(pl.col(\"rds_severity\").is_not_null())\n",
    "fig = px.histogram(phenos_df_filtered.to_pandas(), x=\"rds_severity_int\", nbins=20, title=\"RDS Severity Distribution\")\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b676fe26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import polars as pl\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Assuming 'phenos_df' is your Polars DataFrame with the 'rds_severity_int' column\n",
    "\n",
    "# --- Convert to pandas Series for plotting ---\n",
    "# Matplotlib/Scipy work best with NumPy arrays or pandas Series\n",
    "y = phenos_df[\"rds_severity_int\"].to_pandas().dropna()\n",
    "\n",
    "# --- Create the Q-Q Plot ---\n",
    "plt.style.use('seaborn-v0_8-whitegrid') # Optional: makes the plot look nice\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "# The core of the plot\n",
    "stats.probplot(y, dist=\"norm\", plot=ax)\n",
    "\n",
    "# --- Add informative labels ---\n",
    "ax.set_title(\"Q-Q Plot of Inverse Normal Transformed Severity\", fontsize=16)\n",
    "ax.set_xlabel(\"Theoretical Quantiles (Standard Normal)\", fontsize=12)\n",
    "ax.set_ylabel(\"Sample Quantiles (rds_severity_int)\", fontsize=12)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dca8bea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "sns.kdeplot(data=phenos_df.to_pandas(), x=\"rds_severity_int\", fill=True)\n",
    "plt.title(\"Density Plot of Transformed Severity\", fontsize=16)\n",
    "plt.xlabel(\"rds_severity_int\", fontsize=12)\n",
    "plt.ylabel(\"Density\", fontsize=12)\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a18166b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rds_severity as we have the int version now\n",
    "phenos_df = phenos_df.drop(\"rds_severity\")\n",
    "# Save to file, as TSV\n",
    "phenos_df.write_csv(\"hoiho_phenotypes_15Sept2025.tsv\", separator=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6797f5ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many RDS status vs not?\n",
    "phenos_df.group_by(\"rds_status\").agg(pl.count()).sort(\"rds_status\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dae650b7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "default",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
